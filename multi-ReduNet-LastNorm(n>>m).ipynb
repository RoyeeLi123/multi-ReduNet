{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is used for GPU\n",
    "# train_set is in R^{m_train x d} and test_set is in R^{m_test x d} , m_train is the size of traing set, m_test is the size of test set, d is dimension of features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data setup\n",
    "X_train=train_set\n",
    "y_train=train_label\n",
    "X_test=test_set\n",
    "y_test=test_label\n",
    "num_classes=\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "class Architecture:\n",
    "    def __init__(self, blocks, model_dir, num_classes, batch_size=100):\n",
    "        self.blocks = blocks\n",
    "        self.model_dir = model_dir\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "    def __call__(self, Z, y=None):\n",
    "        for b, block in enumerate(self.blocks):\n",
    "            block.load_arch(self, b)\n",
    "            self.init_loss()\n",
    "\n",
    "            Z = block.preprocess(Z)\n",
    "            Z = block(Z, y)\n",
    "            Z = block.postprocess(Z)\n",
    "        return Z\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.blocks[i]\n",
    "\n",
    "    def init_loss(self):\n",
    "        self.loss_dict = {\"loss_total\": [],\n",
    "                          \"loss_expd\": [],\n",
    "                          \"loss_comp\": []}\n",
    "\n",
    "    def update_loss(self, layer, loss_total, loss_expd, loss_comp):\n",
    "        self.loss_dict[\"loss_total\"].append(loss_total)\n",
    "        self.loss_dict[\"loss_expd\"].append(loss_expd)\n",
    "        self.loss_dict[\"loss_comp\"].append(loss_comp)\n",
    "        print(f\"layer: {layer} | loss_total: {loss_total:5f} | loss_expd: {loss_expd:5f} | loss_comp: {loss_comp:5f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##utils\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "\n",
    "def sort_dataset(data, labels, classes, stack=False):\n",
    "    \"\"\"Sort dataset based on classes.\n",
    "\n",
    "    Parameters:\n",
    "        data (np.ndarray): data array\n",
    "        labels (np.ndarray): one dimensional array of class labels\n",
    "        classes (int): number of classes\n",
    "        stack (bol): combine sorted data into one numpy array\n",
    "\n",
    "    Return:\n",
    "        sorted data (np.ndarray), sorted_labels (np.ndarray)\n",
    "\n",
    "    \"\"\"\n",
    "    if type(classes) == int:\n",
    "        classes = cp.arange(classes)\n",
    "    sorted_data = []\n",
    "    sorted_labels = []\n",
    "    for c in classes:\n",
    "        idx = (labels == c)\n",
    "        data_c = data[idx]\n",
    "        labels_c = labels[idx]\n",
    "        sorted_data.append(data_c)\n",
    "        sorted_labels.append(labels_c)\n",
    "    if stack:\n",
    "        sorted_data = cp.vstack(sorted_data)\n",
    "        sorted_labels = cp.hstack(sorted_labels)\n",
    "    return sorted_data, sorted_labels\n",
    "\n",
    "def save_params(model_dir, params, name='params.json'):\n",
    "    \"\"\"Save params to a .json file. Params is a dictionary of parameters.\"\"\"\n",
    "    path = os.path.join(model_dir, name)\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(params, f, indent=2, sort_keys=True)\n",
    "\n",
    "def load_params(model_dir):\n",
    "    \"\"\"Load params.json file in model directory and return dictionary.\"\"\"\n",
    "    _path = os.path.join(model_dir, \"params.json\")\n",
    "    with open(_path, 'r') as f:\n",
    "        _dict = json.load(f)\n",
    "    return _dict\n",
    "\n",
    "def create_csv(model_dir, filename, headers):\n",
    "    \"\"\"Create .csv file with filename in model_dir, with headers as the first line\n",
    "    of the csv. \"\"\"\n",
    "    csv_path = os.path.join(model_dir, filename)\n",
    "    if os.path.exists(csv_path):\n",
    "        os.remove(csv_path)\n",
    "    with open(csv_path, 'w+') as f:\n",
    "        f.write(','.join(map(str, headers)))\n",
    "    return csv_path\n",
    "\n",
    "def save_loss(loss_dict, model_dir, name):\n",
    "    save_dir = os.path.join(model_dir, \"loss\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    file_path = os.path.join(save_dir, \"{}.csv\".format(name))\n",
    "    pd.DataFrame(loss_dict).to_csv(file_path)\n",
    "\n",
    "def save_features(model_dir, name, features, labels, layer=None):\n",
    "    save_dir = os.path.join(model_dir, \"features\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    cp.save(os.path.join(save_dir, f\"{name}_features.npy\"), features)\n",
    "    cp.save(os.path.join(save_dir, f\"{name}_labels.npy\"), labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#functionals\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def get_n_each(X, y, n, b=0):\n",
    "    classes = np.unique(y)\n",
    "    _X, _y = [], []\n",
    "    for c in classes:\n",
    "        idx = y==c\n",
    "        X_class = X[idx][b*n:(b+1)*n]\n",
    "        y_class = y[idx][:n]\n",
    "        _X.append(X_class)\n",
    "        _y.append(y_class)\n",
    "    return np.vstack(_X), np.hstack(_y)\n",
    "\n",
    "def translate1d(data, labels, n=None, stride=1):\n",
    "    n_samples, _, n_dim = data.shape\n",
    "    data_new = []\n",
    "    if n is None:\n",
    "        shifts = np.arange(0, n_dim, stride)\n",
    "    else:\n",
    "        shifts = np.arange(-n*stride, (n+1)*stride, stride)\n",
    "    for r in shifts:\n",
    "        data_new.append(np.roll(data, r, axis=2))\n",
    "    return (np.vstack(data_new),\n",
    "            np.tile(labels, len(shifts)))\n",
    "\n",
    "def translate2d(data, labels, n=None, stride=1):\n",
    "    n_samples, _, H, W = data.shape\n",
    "    data_new = []\n",
    "    if n is None:\n",
    "        vshifts = np.arange(0, H, stride)\n",
    "        hshifts = np.arange(0, W, stride)\n",
    "    else:\n",
    "        hshifts = np.arange(-n*stride, (n+1)*stride, stride)\n",
    "        vshifts = np.arange(-n*stride, (n+1)*stride, stride)\n",
    "    for h in vshifts:\n",
    "        for w in hshifts:\n",
    "            data_new.append(np.roll(data, (h, w), axis=(2, 3)))\n",
    "    return (np.vstack(data_new),\n",
    "            np.tile(labels, len(vshifts)*len(hshifts)))\n",
    "\n",
    "def shuffle(data, labels, seed=10):\n",
    "    np.random.seed(seed)\n",
    "    num_samples = data.shape[0]\n",
    "    idx = np.random.choice(np.arange(num_samples), num_samples, replace=False)\n",
    "    return data[idx], labels[idx]\n",
    "\n",
    "def filter_class(data, labels, classes, n=None, b=0):\n",
    "    if type(classes) == int:\n",
    "        classes = np.arange(classes)\n",
    "    data_filter = []\n",
    "    labels_filter = []\n",
    "    for _class in classes:\n",
    "        idx = labels == _class\n",
    "        data_filter.append(data[idx][b*n:(b+1)*n])\n",
    "        labels_filter.append(labels[idx][b*n:(b+1)*n])\n",
    "    data_new = np.vstack(data_filter)\n",
    "    labels_new = np.unique(np.hstack(labels_filter), return_inverse=True)[1]\n",
    "    return data_new, labels_new\n",
    "\n",
    "def normalize(X, p=2):\n",
    "    axes = tuple(np.arange(1, len(X.shape)).tolist())\n",
    "    norm = cp.linalg.norm(X.reshape(X.shape[0], -1), axis=1, ord=p)\n",
    "    norm = cp.clip(norm, 1e-8, np.inf)\n",
    "    return X / cp.expand_dims(norm, axes)\n",
    "\n",
    "def batch_cov(V, bs):\n",
    "    m = V.shape[0]\n",
    "    return np.sum([np.einsum('ji...,jk...->ik...', V[i:i+bs], V[i:i+bs].conj(), optimize=True) \\\n",
    "                     for i in np.arange(0, m, bs)], axis=0)\n",
    "\n",
    "def generate_kernel(mode, size, seed=10):\n",
    "    np.random.seed(seed)\n",
    "    if mode == 'gaussian':\n",
    "        return np.random.normal(0., 1., size=size)\n",
    "    elif mode == 'ones':\n",
    "        return np.ones(size=size)\n",
    "\n",
    "def convert2polar(images, channels, timesteps):\n",
    "    mid_pt = images.shape[1] // 2\n",
    "    r = np.linspace(0, mid_pt, channels).astype(np.int32)\n",
    "    angles = np.linspace(0, 360, timesteps)\n",
    "    polar_imgs = []\n",
    "    for angle in angles:\n",
    "        X_rot = scipy.ndimage.rotate(images, angle, axes=(1, 2), reshape=False)\n",
    "        polar_imgs.append(X_rot[:, mid_pt, r])\n",
    "    polar_imgs = np.stack(polar_imgs).transpose(1, 2, 0)\n",
    "    return polar_imgs\n",
    "\n",
    "\n",
    "##evaluate\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def svm(train_features, train_labels, test_features, test_labels):\n",
    "    svm = LinearSVC(verbose=0, random_state=10)\n",
    "    svm.fit(train_features, train_labels)\n",
    "    acc_train = svm.score(train_features, train_labels)\n",
    "    acc_test = svm.score(test_features, test_labels)\n",
    "    print(\"SVM: {}\".format(acc_test))\n",
    "    return acc_train, acc_test\n",
    "\n",
    "def knn(train_features, train_labels, test_features, test_labels, k=5):\n",
    "    \"\"\"Perform k-Nearest Neighbor classification using cosine similaristy as metric.\n",
    "    Options:\n",
    "        k (int): top k features for kNN\n",
    "\n",
    "    \"\"\"\n",
    "    sim_mat = train_features @ test_features.T\n",
    "    topk = torch.from_numpy(sim_mat).topk(k=k, dim=0)\n",
    "    topk_pred = train_labels[topk.indices]\n",
    "    test_pred = torch.tensor(topk_pred).mode(0).values.detach()\n",
    "\n",
    "    #print(\"knn\",test_pred[0])\n",
    "\n",
    "    acc = compute_accuracy(test_pred.numpy(), test_labels)\n",
    "    print(\"kNN: {}\".format(acc))\n",
    "    return acc\n",
    "\n",
    "def nearsub(train_features, train_labels, test_features, test_labels, n_comp=10):\n",
    "    \"\"\"Perform nearest subspace classification.\n",
    "\n",
    "    Options:\n",
    "        n_comp (int): number of components for PCA or SVD\n",
    "\n",
    "    \"\"\"\n",
    "    scores_svd = []\n",
    "    classes = np.unique(test_labels)\n",
    "    features_sort, _ = sort_dataset(train_features, train_labels,\n",
    "                                          classes=classes, stack=False)\n",
    "    fd = features_sort[0].shape[1]\n",
    "    if n_comp >= fd:\n",
    "        n_comp = fd - 1\n",
    "    for j in np.arange(len(classes)):\n",
    "        svd = TruncatedSVD(n_components=n_comp).fit(features_sort[j])\n",
    "        svd_subspace = svd.components_.T\n",
    "        svd_j = (np.eye(fd) - svd_subspace @ svd_subspace.T) \\\n",
    "                        @ (test_features).T\n",
    "        score_svd_j = np.linalg.norm(svd_j, ord=2, axis=0)\n",
    "        scores_svd.append(score_svd_j)\n",
    "    test_predict_svd = np.argmin(scores_svd, axis=0)\n",
    "    ###\n",
    "    #print('predict_svd',test_predict_svd[0])\n",
    "    ###\n",
    "    acc_svd = compute_accuracy(classes[test_predict_svd], test_labels)\n",
    "    print('SVD: {}'.format(acc_svd))\n",
    "    return acc_svd\n",
    "\n",
    "def nearsub_pca(train_features, train_labels, test_features, test_labels, n_comp=10):\n",
    "    \"\"\"Perform nearest subspace classification.\n",
    "\n",
    "    Options:\n",
    "        n_comp (int): number of components for PCA or SVD\n",
    "\n",
    "    \"\"\"\n",
    "    scores_pca = []\n",
    "    classes = np.unique(test_labels)\n",
    "    features_sort, _ = sort_dataset(train_features, train_labels,\n",
    "                                          classes=classes, stack=False)\n",
    "    fd = features_sort[0].shape[1]\n",
    "    if n_comp >= fd:\n",
    "        n_comp = fd - 1\n",
    "    for j in np.arange(len(classes)):\n",
    "        pca = PCA(n_components=n_comp).fit(features_sort[j])\n",
    "        pca_subspace = pca.components_.T\n",
    "        mean = np.mean(features_sort[j], axis=0)\n",
    "        pca_j = (np.eye(fd) - pca_subspace @ pca_subspace.T) \\\n",
    "                        @ (test_features - mean).T\n",
    "        score_pca_j = np.linalg.norm(pca_j, ord=2, axis=0)\n",
    "        scores_pca.append(score_pca_j)\n",
    "    test_predict_pca = np.argmin(scores_pca, axis=0)\n",
    "    acc_pca = compute_accuracy(classes[test_predict_pca], test_labels)\n",
    "    print('PCA: {}'.format(acc_pca))\n",
    "    return acc_svd\n",
    "\n",
    "def compute_accuracy(y_pred, y_true):\n",
    "    \"\"\"Compute accuracy by counting correct classification. \"\"\"\n",
    "    assert y_pred.shape == y_true.shape\n",
    "    return 1 - np.count_nonzero(y_pred - y_true) / y_true.size\n",
    "\n",
    "def baseline(train_features, train_labels, test_features, test_labels):\n",
    "    test_models = {'log_l2': SGDClassifier(loss='log', max_iter=10000, random_state=42),\n",
    "                   'SVM_linear': LinearSVC(max_iter=10000, random_state=42),\n",
    "                   'SVM_RBF': SVC(kernel='rbf', random_state=42),\n",
    "                   'DecisionTree': DecisionTreeClassifier(),\n",
    "                   'RandomForrest': RandomForestClassifier()}\n",
    "    for model_name in test_models:\n",
    "        test_model = test_models[model_name]\n",
    "        test_model.fit(train_features, train_labels)\n",
    "        score = test_model.score(test_features, test_labels)\n",
    "        print(f\"{model_name}: {score}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Vector\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "class Vector:\n",
    "    def __init__(self, layers, eta, eps, lmbda=500):\n",
    "        self.layers = layers\n",
    "        self.eta = eta\n",
    "        self.eps = eps\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "    def __call__(self, Z, y=None):\n",
    "        for layer in range(self.layers):\n",
    "            Z, y_approx = self.forward(layer, Z, y)\n",
    "            self.arch.update_loss(layer, *self.compute_loss(Z, y_approx))\n",
    "        return Z\n",
    "\n",
    "    def forward(self, layer, Z, y=None):\n",
    "        if y is not None:\n",
    "            self.feature=Z\n",
    "            self.label=y\n",
    "            self.init(Z, y)\n",
    "            self.save_weights(layer)\n",
    "            self.save_gam(layer)\n",
    "\n",
    "            m,d=Z.shape\n",
    "            c=d/(m*self.eps)\n",
    "            for j in range(self.num_classes):\n",
    "                Z_j=Z[y==j]\n",
    "                m_j=Z_j.shape[0]\n",
    "                c_j=d/(m_j*self.eps)\n",
    "                pre_Ej=cp.linalg.inv(cp.eye(m_j)+c * Z_j @ Z_j.T)\n",
    "                pre_Cj=cp.linalg.inv(cp.eye(m_j)+c_j* Z_j@ Z_j.T)\n",
    "                E_j=c*(cp.eye(d)-c * Z_j.T @ pre_Ej @ Z_j)\n",
    "                C_j=c_j*(cp.eye(d)-c_j* Z_j.T @ pre_Cj @ Z_j)\n",
    "                expd=Z_j @ E_j.T\n",
    "                clus=self.gam[j] * Z_j @ C_j.T\n",
    "                Z_j=Z_j+self.eta*(expd-clus)\n",
    "                if j==0:\n",
    "                    out=Z_j\n",
    "                else:\n",
    "                    out=cp.vstack((out,Z_j))\n",
    "            if layer==self.layers-1:\n",
    "                out=normalize(out)\n",
    "            return out,y\n",
    "\n",
    "\n",
    "        else:\n",
    "            self.load_weights(layer)\n",
    "            self.load_gam(layer)\n",
    "\n",
    "            m, d =self.feature.shape\n",
    "            c=d / (m * self.eps)\n",
    "            for j in range(self.num_classes):\n",
    "                m_j=self.feature[self.label==j].shape[0]\n",
    "                c_j=d/(m_j * self.eps)\n",
    "                pre_Cj=cp.linalg.inv(cp.eye(m_j)+ c_j * self.feature[self.label==j] @ self.feature[self.label==j].T)\n",
    "                C_j=c_j*(cp.eye(d)-c_j* self.feature[self.label==j].T @ pre_Cj @ self.feature[self.label==j])\n",
    "                if j==0:\n",
    "                    comp= Z @ C_j.T\n",
    "                else:\n",
    "                    comp=cp.vstack((comp, Z @ C_j.T))\n",
    "            m_1=Z.shape[0]\n",
    "            comp=comp.reshape((self.num_classes,m_1,d))\n",
    "            pred_pi, y_approx = self.nonlinear(comp)\n",
    "            for j in range(self.num_classes):\n",
    "                m_j=self.feature[self.label==j].shape[0]\n",
    "                pre_Ej=cp.linalg.inv(cp.eye(m_j)+ c * self.feature[self.label==j] @ self.feature[self.label==j].T)\n",
    "                E_j=c*(cp.eye(d)-c* self.feature[self.label==j].T @ pre_Ej @ self.feature[self.label==j])\n",
    "                Z_j=Z+ self.eta * (Z@ E_j.T-self.gam[j]* comp[j])\n",
    "                Z_j=pred_pi[j]*Z_j\n",
    "                if j==0:\n",
    "                    out=Z_j\n",
    "                else:\n",
    "                    out=out+Z_j\n",
    "            if layer==self.layers-1:\n",
    "                out=normalize(out)\n",
    "            return out,y_approx\n",
    "\n",
    "\n",
    "    def first_ortho(self,Z,y):\n",
    "        y_1=np.array([])\n",
    "        Z_0=Z[y==0]\n",
    "        Z_0=Z_0.T\n",
    "        y_1=np.concatenate((y_1, np.array([int(0)]*Z_0.shape[1])), axis=0)\n",
    "        U_0,R_0=cp.linalg.qr(Z_0, mode='reduced')\n",
    "        A=U_0\n",
    "        output=Z_0\n",
    "        for j in np.arange(1,self.num_classes):\n",
    "            if j==1:\n",
    "                A=U_0\n",
    "            else:\n",
    "                A=cp.concatenate((A,B ), axis=1)\n",
    "            U=A @ A.T\n",
    "            Z_j=Z[y==j].T\n",
    "            y_1=np.concatenate((y_1, np.array([int(j)]*Z_j.shape[1])), axis=0)\n",
    "            Z_j=(cp.eye(U.shape[0])-U)@ Z_j\n",
    "            B,R_j=cp.linalg.qr(Z_j, mode='reduced')\n",
    "            output=cp.hstack((output, Z_j))\n",
    "        output=output.T\n",
    "        return output,y_1\n",
    "\n",
    "\n",
    "    def load_arch(self, arch, block_id):\n",
    "        self.arch = arch\n",
    "        self.block_id = block_id\n",
    "        self.num_classes = self.arch.num_classes\n",
    "\n",
    "    def init(self, Z, y):\n",
    "        self.compute_gam(y)\n",
    "\n",
    "\n",
    "    def compute_gam(self, y):\n",
    "        m_j = [(y==j).nonzero()[0].size for j in range(self.num_classes)]\n",
    "        self.gam = np.array(m_j) / y.size\n",
    "\n",
    "\n",
    "\n",
    "    def compute_loss(self, Z, y):\n",
    "        m, d = Z.shape\n",
    "        I = cp.eye(d)\n",
    "\n",
    "        c = d / (m * self.eps)\n",
    "        logdet = cp.linalg.slogdet(I + c * Z.T @ Z)[1]\n",
    "        loss_expd = logdet / 2.\n",
    "\n",
    "        loss_comp = 0.\n",
    "        for j in np.arange(self.num_classes):\n",
    "            idx = (y == int(j))\n",
    "            Z_j = Z[idx, :]\n",
    "            m_j = Z_j.shape[0]\n",
    "            if m_j == 0:\n",
    "                continue\n",
    "            c_j = d / (m_j * self.eps)\n",
    "            logdet_j = cp.linalg.slogdet(I + c_j * Z_j.T @ Z_j)[1]\n",
    "            loss_comp += self.gam[j] * logdet_j / 2.\n",
    "        loss_expd=cp.asnumpy(loss_expd)\n",
    "        loss_comp=cp.asnumpy(loss_comp)\n",
    "        return loss_expd - loss_comp, loss_expd, loss_comp\n",
    "\n",
    "    def preprocess(self, X):\n",
    "        m = X.shape[0]\n",
    "        X = X.reshape(m, -1)\n",
    "        return normalize(X)\n",
    "\n",
    "    def postprocess(self, X):\n",
    "        return normalize(X)\n",
    "\n",
    "\n",
    "    def nonlinear(self, Bz):\n",
    "        axes = tuple(np.arange(2, len(Bz.shape)))\n",
    "        norm = cp.linalg.norm(Bz.reshape(Bz.shape[0], Bz.shape[1], -1), axis=2)\n",
    "        norm = cp.clip(norm, 1e-8, norm)\n",
    "        norm=cp.asnumpy(norm)\n",
    "        pred = softmax(-self.lmbda * norm, axis=0)\n",
    "        pred=cp.asarray(pred)\n",
    "        #print(pred[:,0])\n",
    "        y = cp.argmax(pred, axis=0)\n",
    "\n",
    "        return cp.expand_dims(pred, axes), y\n",
    "\n",
    "    def save_weights(self, layer):\n",
    "        weight_dir = os.path.join(self.arch.model_dir, \"weights\")\n",
    "        os.makedirs(weight_dir, exist_ok=True)\n",
    "        save_path = os.path.join(weight_dir, f\"{self.block_id}_{layer}.npz\")\n",
    "        cp.savez(save_path, array1=self.feature,array2=self.label)\n",
    "\n",
    "    def load_weights(self, layer):\n",
    "        weight_dir = os.path.join(self.arch.model_dir, \"weights\")\n",
    "        save_path = os.path.join(weight_dir, f\"{self.block_id}_{layer}.npz\")\n",
    "        weights = cp.load(save_path)\n",
    "        self.feature=weights['array1']\n",
    "        self.label=weights['array2']\n",
    "        return self.feature, self.label\n",
    "\n",
    "    def save_gam(self, layer):\n",
    "        weight_dir = os.path.join(self.arch.model_dir, \"weights\")\n",
    "        os.makedirs(weight_dir, exist_ok=True)\n",
    "        save_path = os.path.join(weight_dir, f\"{self.block_id}_{layer}_gam.npy\")\n",
    "        np.save(save_path, self.gam)\n",
    "\n",
    "    def load_gam(self, layer):\n",
    "        weight_dir = os.path.join(self.arch.model_dir, \"weights\")\n",
    "        save_path = os.path.join(weight_dir, f\"{self.block_id}_{layer}_gam.npy\")\n",
    "        self.gam = np.load(save_path)\n",
    "        return self.gam\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######\n",
    "#layers=10\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--layers', type=int, default=10, help=\"number of layers\")\n",
    "parser.add_argument('--eta', type=float, default=0.2, help='learning rate')\n",
    "parser.add_argument('--eps', type=float, default=0.1, help='eps squared')\n",
    "parser.add_argument('--tail', type=str, default='',\n",
    "                    help='extra information to add to folder name')\n",
    "parser.add_argument('--save_dir', type=str, default='./saved_models/',\n",
    "                    help='base directory for saving PyTorch model. (default: ./saved_models/)')\n",
    "parser.add_argument('--data_dir', type=str, default='./data/',\n",
    "                    help='base directory for saving PyTorch model. (default: ./saved_models/)')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# pipeline setup\n",
    "model_dir = os.path.join(\"./saved_models\", \"multi-ReduNet-LastNorm\",\n",
    "                         \"layers{}_eps{}_eta{}\"\n",
    "                         \"\".format(args.layers, args.eps, args.eta)\n",
    "                         )\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "save_params(model_dir, vars(args))\n",
    "print(model_dir)\n",
    "\n",
    "\n",
    "\n",
    "# model setup\n",
    "layers = [Vector(args.layers, eta=args.eta, eps=args.eps)]\n",
    "model = Architecture(layers, model_dir, num_classes)\n",
    "\n",
    "\n",
    "# train/test pass\n",
    "print(\"Forward pass - train features\")\n",
    "start_time=time.time()\n",
    "\n",
    "Z_train = model(X_train, y_train)\n",
    "\n",
    "end_time=time.time()\n",
    "\n",
    "save_loss(model.loss_dict, model_dir, \"train\")\n",
    "print(\"Forward pass - test features\")\n",
    "Z_test = model(X_test)\n",
    "save_loss(model.loss_dict, model_dir, \"test\")\n",
    "\n",
    "# save features\n",
    "save_features(model_dir, \"X_train\", X_train, y_train)\n",
    "save_features(model_dir, \"X_test\", X_test, y_test)\n",
    "save_features(model_dir, \"Z_train\", Z_train, y_train)\n",
    "save_features(model_dir, \"Z_test\", Z_test, y_test)\n",
    "\n",
    "# evaluation\n",
    "Z_train=cp.asnumpy(Z_train)\n",
    "y_train=cp.asnumpy(y_train)\n",
    "Z_test=cp.asnumpy(Z_test)\n",
    "y_test=cp.asnumpy(y_test)\n",
    "\n",
    "_, acc_svm = svm(Z_train, y_train, Z_test, y_test)\n",
    "acc_knn = knn(Z_train, y_train, Z_test, y_test, k=5)\n",
    "acc_svd = nearsub(Z_train, y_train, Z_test, y_test, n_comp=5)\n",
    "acc = {\"svm\": acc_svm, \"knn\": acc_knn, \"nearsub-svd\": acc_svd}\n",
    "save_params(model_dir, acc, name=\"acc_test.json\")\n",
    "\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Model execution time: {elapsed_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import normalize\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "\n",
    "\n",
    "def plot_heatmap(features, labels, title, model_dir):\n",
    "    \"\"\"Plot heatmap of cosine simliarity for all features. \"\"\"\n",
    "    num_samples = features.shape[0]\n",
    "    classes = np.arange(np.unique(labels).size)\n",
    "    features_sort_, _ = sort_dataset(features, labels,\n",
    "                            classes=classes, stack=True)\n",
    "    sim_mat = np.abs(features_sort_ @ features_sort_.T)\n",
    "    print(sim_mat.min(), sim_mat.max())\n",
    "\n",
    "#    plt.rc('text', usetex=True)\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "    plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "    fig, ax = plt.subplots(figsize=(8, 7), sharey=True, sharex=True)\n",
    "    im = ax.imshow(sim_mat, cmap='Blues')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "    cbar = fig.colorbar(im, cax=cax, drawedges=0, ticks=[0, 0.5, 1])\n",
    "    cbar.ax.tick_params(labelsize=18)\n",
    "    # fig.colorbar(im, pad=0.02, drawedges=0, ticks=[0, 0.5, 1])\n",
    "    ax.set_xticks(np.linspace(0, num_samples, len(classes)+1))\n",
    "    ax.set_yticks(np.linspace(0, num_samples, len(classes)+1))\n",
    "    [tick.label.set_fontsize(24) for tick in ax.xaxis.get_major_ticks()]\n",
    "    [tick.label.set_fontsize(24) for tick in ax.yaxis.get_major_ticks()]\n",
    "    fig.tight_layout()\n",
    "\n",
    "    save_dir = os.path.join(model_dir, \"figures\", \"heatmaps\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(save_dir, f\"heatmap-{title}.pdf\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_combined_loss(model_dir, update=None):\n",
    "    \"\"\"Plot theoretical loss and empirical loss.\n",
    "\n",
    "    Figure 3: gaussian2d, gaussian3d, fontsize 24\n",
    "\n",
    "    \"\"\"\n",
    "#    plt.rc('text', usetex=True)\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "    plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(7, 5), sharey=True, sharex=True)\n",
    "    models = ['train', 'test']\n",
    "    linestyles = ['solid', 'dashed']\n",
    "    markers = ['o', 'D']\n",
    "    markersizes = [4.5, 3]\n",
    "    alphas = [0.5, 0.9]\n",
    "    names = ['$\\Delta R$ ', '$R$', '$R_c$']\n",
    "    colors = ['green', 'royalblue', 'coral']\n",
    "    for model, linestyle, marker, alpha, markersize in zip(models, linestyles, markers, alphas, markersizes):\n",
    "        filename = os.path.join(model_dir, \"loss\", f'{model}.csv')\n",
    "        data = pd.read_csv(filename)\n",
    "        losses = [data['loss_total'].ravel(), data['loss_expd'].ravel(), data['loss_comp'].ravel()]\n",
    "        for loss, name, color in zip(losses, names, colors):\n",
    "            num_iter = np.arange(loss.size)\n",
    "            ax.plot(num_iter, loss, label=r'{} ({})'.format(name, model),\n",
    "                color=color, linewidth=1.5, alpha=alpha, linestyle=linestyle,\n",
    "                marker=marker, markersize=markersize, markevery=5, markeredgecolor='black')\n",
    "    ax.set_ylabel('Loss', fontsize=40)\n",
    "    ax.set_xlabel('Layers', fontsize=40)\n",
    "    # ax.set_ylim((-0.05, 2.8)) # gaussian2d\n",
    "    # ax.set_yticks(np.linspace(0, 2.5, 6)) # gaussian2d\n",
    "    # ax.set_ylim((-0.05, 2.5)) # gaussian2d\n",
    "    # ax.set_yticks(np.linspace(0, 2.5, 6)) # gaussian2d\n",
    "    # ax.set_ylim((0, 4.0)) # gaussian3d\n",
    "    # ax.set_yticks(np.linspace(0, 4.0, 9)) # gaussian3d\n",
    "    # ax.set_ylim((-0.005, 0.075)) # mnist_rotation_classes01\n",
    "    # ax.set_yticks(np.linspace(0, 0.075, 6)) # mnist_rotation_classes01\n",
    "    # ax.set_ylim((-0.02, 0.1)) # sinusoid\n",
    "    # ax.set_yticks(np.linspace(0, 0.1, 5)) # sinusoid\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    handles = [handles[i] for i in [0, 3, 1, 4, 2, 5]]\n",
    "    labels = [labels[i] for i in [0, 3, 1, 4, 2, 5]]\n",
    "    ax.legend(handles, labels, loc='lower right', prop={\"size\": 13}, ncol=3, framealpha=0.5)\n",
    "    [tick.label.set_fontsize(22) for tick in ax.xaxis.get_major_ticks()]\n",
    "    [tick.label.set_fontsize(22) for tick in ax.yaxis.get_major_ticks()]\n",
    "    fig.tight_layout()\n",
    "\n",
    "    save_dir = os.path.join(model_dir, 'figures', 'loss')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    file_name = os.path.join(save_dir, f'loss-traintest.pdf')\n",
    "    plt.savefig(file_name, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def plot_2d(Z, y, name, model_dir):\n",
    "    plot_dir = os.path.join(model_dir, \"figures\", \"2dscatter\")\n",
    "    colors = np.array(['forestgreen', 'red', 'royalblue', 'purple', 'darkblue', 'orange'])\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "#    plt.rc('text', usetex=True)\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "    plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "    # colors = np.array(['royalblue', 'forestgreen', 'red'])\n",
    "    fig, ax = plt.subplots(figsize=(6, 5), dpi=200)\n",
    "    ax.scatter(Z[:, 0], Z[:, 1], c=colors[y], alpha=0.5)\n",
    "    ax.scatter(0.0, 0.0, c='black', alpha=0.8, marker='s')\n",
    "    # ax.arrow(0.0, 0.0, Z[:, 0], Z[:, 1])\n",
    "    ax.set_ylim(-1.2, 1.2)\n",
    "    ax.set_xlim(-1.2, 1.2)\n",
    "    ax.set_xticks([-1.0, -0.5, 0.0, 0.5, 1.0])\n",
    "    ax.set_yticks([-1.0, -0.5, 0.0, 0.5, 1.0])\n",
    "    ax.grid(linestyle=':')\n",
    "    Z, _ = F.get_n_each(Z, y, 1)\n",
    "    for c in np.unique(y):\n",
    "        ax.arrow(0, 0, Z[c, 0], Z[c, 1], head_width=0.03, head_length=0.05, fc='k', ec='k', length_includes_head=True)\n",
    "    [tick.label.set_fontsize(24) for tick in ax.xaxis.get_major_ticks()]\n",
    "    [tick.label.set_fontsize(24) for tick in ax.yaxis.get_major_ticks()]\n",
    "    plt.savefig(os.path.join(plot_dir, \"scatter2d-\"+name+\".pdf\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def plot_3d(Z, y, name, model_dir):\n",
    "    colors = np.array(['green', 'blue', 'red'])\n",
    "    savedir = os.path.join(model_dir, 'figures', '3d')\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "#    plt.rc('text', usetex=True)\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "    plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "    colors = np.array(['forestgreen', 'royalblue', 'brown'])\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(Z[:, 0], Z[:, 1], Z[:, 2], c=colors[y], cmap=plt.cm.Spectral, s=200.0)\n",
    "    Z, _ = F.get_n_each(Z, y, 1)\n",
    "    for c in np.unique(y):\n",
    "        ax.quiver(0.0, 0.0, 0.0, Z[c, 0], Z[c, 1], Z[c, 2], length=1.0, normalize=True, arrow_length_ratio=0.05, color='black')\n",
    "    u, v = np.mgrid[0:2*np.pi:20j, 0:np.pi:10j]\n",
    "    x = np.cos(u)*np.sin(v)\n",
    "    y = np.sin(u)*np.sin(v)\n",
    "    z = np.cos(v)\n",
    "    ax.plot_wireframe(x, y, z, color=\"gray\", alpha=0.5)\n",
    "    ax.xaxis._axinfo[\"grid\"]['color'] =  (0,0,0,0.1)\n",
    "    ax.yaxis._axinfo[\"grid\"]['color'] =  (0,0,0,0.1)\n",
    "    ax.zaxis._axinfo[\"grid\"]['color'] =  (0,0,0,0.1)\n",
    "    [tick.label.set_fontsize(24) for tick in ax.xaxis.get_major_ticks()]\n",
    "    [tick.label.set_fontsize(24) for tick in ax.yaxis.get_major_ticks()]\n",
    "    [tick.label.set_fontsize(24) for tick in ax.zaxis.get_major_ticks()]\n",
    "    ax.view_init(20, 15)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(savedir, f\"scatter3d-{name}.jpg\"), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def plot_sample_angle_combined(train_features, train_labels, test_features, test_labels, model_dir, title1, title2, tail=\"\"):\n",
    "    save_dir = os.path.join(model_dir, \"figures\", \"sample_angle_combined\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    _bins = np.linspace(-0.05, 1.05, 21)\n",
    "\n",
    "    classes = np.unique(y_train)\n",
    "    fs_train, _ = sort_dataset(train_features, train_labels,\n",
    "                        classes=classes, stack=False)\n",
    "    fs_test, _ = sort_dataset(test_features, test_labels,\n",
    "                            classes=classes, stack=False)\n",
    "    angles = []\n",
    "    for class_train in classes:\n",
    "        for class_test in classes:\n",
    "            if class_train == class_test:\n",
    "                continue\n",
    "            angles.append((fs_train[class_train] @ fs_test[class_test].T).reshape(-1))\n",
    "\n",
    "#    plt.rc('text', usetex=True)\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "    plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    ax.hist(np.hstack(angles), bins=_bins, alpha=0.5,   color='red', #colors[class_test],\n",
    "                edgecolor='black')#, label=f'Class {class_test}')\n",
    "    ax.set_xlabel('Similarity', fontsize=38)\n",
    "    ax.set_ylabel('Count', fontsize=38)\n",
    "    ax.ticklabel_format(style='sci', scilimits=(0, 3))\n",
    "    [tick.label.set_fontsize(22) for tick in ax.xaxis.get_major_ticks()]\n",
    "    [tick.label.set_fontsize(22) for tick in ax.yaxis.get_major_ticks()]\n",
    "    # ax.legend(loc='upper center', prop={\"size\": 13}, ncol=1, framealpha=0.5)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(save_dir, f'sample_angle_combined-{title1}-vs-{title2}{tail}.pdf'))\n",
    "    plt.close()\n",
    "\n",
    "plot_combined_loss(model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
